# Building a Simple Generative Adversarial Network (GAN) using PyTorch

This project demonstrates the creation and training of a basic Generative Adversarial Network (GAN) using PyTorch. The GAN is trained on the CIFAR-10 dataset to generate small, colorful images.

This notebook is designed for educational purposes, providing a hands-on introduction to GANs for those new to the concept or looking for a simple PyTorch implementation. It's intended to be run cell by cell in a Google Colab environment.

## ‚ú® What it Does
*   Implements a basic **Generative Adversarial Network (GAN)** with a Generator and a Discriminator.
*   Uses **PyTorch** for building and training the neural networks.
*   Trains on the **CIFAR-10 dataset**, which consists of 32x32 color images in 10 classes.
*   The **Generator** learns to create images that resemble the CIFAR-10 dataset from random noise.
*   The **Discriminator** learns to distinguish between real images (from CIFAR-10) and fake images (generated by the Generator).
*   Designed for easy, cell-by-cell execution in **Google Colab**.
*   Optimized for use with a **T4 GPU** on Colab (though other GPUs might work).
*   Outputs generated image samples at the end of training.

## üõ†Ô∏è Prerequisites
*   A Google Account (for accessing Google Colab).
*   Basic understanding of neural networks and PyTorch concepts is helpful but not strictly necessary to run the demo.

## üöÄ How to Use
1.  **Open in Colab:**
    *   Upload the `Building_a_simple_Generative_Adversarial_Network_(GAN)_using_PyTorch.ipynb` notebook to your Google Drive.
    *   Open it with Google Colaboratory.
    *   Alternatively, if you upload this project to GitHub, you can directly open the notebook in Colab using the "Open in Colab" badge.

2.  **Set Runtime Type:**
    *   In Colab, navigate to `Runtime` -> `Change runtime type`.
    *   Under `Hardware accelerator`, select `T4 GPU` (or another available GPU).
    *   Click `Save`.

3.  **Run Cells Sequentially:**
    *   Execute each code cell in the notebook from top to bottom by clicking the "Play" button to the left of each cell or by selecting a cell and pressing `Shift + Enter`.
    *   **Imports Cell:** Imports necessary libraries (PyTorch, Torchvision, Matplotlib, NumPy).
    *   **Transform & Dataloader Cells:** Defines image transformations (ToTensor, Normalize) and downloads/loads the CIFAR-10 dataset. The dataset will be downloaded automatically on the first run.
    *   **Hyperparameters Cell:** Sets key parameters like latent dimension, learning rate, and number of epochs.
    *   **Generator & Discriminator Class Cells:** Defines the neural network architectures for both the Generator and the Discriminator.
    *   **Initialization Cell:** Initializes the Generator and Discriminator, moves them to the selected device (GPU/CPU), defines the loss function (Binary Cross-Entropy), and sets up Adam optimizers.
    *   **Training Loop Cell:** This is the core of the GAN training.
        *   It iterates for the specified number of `num_epochs`.
        *   In each epoch, it processes batches of real images from the CIFAR-10 dataset.
        *   **Discriminator Training:** The Discriminator is trained to correctly classify real images as real and fake images (generated by the Generator from random noise) as fake.
        *   **Generator Training:** The Generator is trained to produce images that the Discriminator misclassifies as real.
        *   Losses for both networks are printed every 100 batches.
        *   At the end of every 10 epochs (in this case, at the end of the 10th epoch as per `num_epochs`), a grid of 16 generated images is displayed using Matplotlib.

## üìä Dataset
The project uses the **CIFAR-10 dataset**.
*   It consists of 60,000 32x32 color images in 10 classes (e.g., airplane, automobile, bird, cat).
*   The notebook automatically downloads this dataset if it's not found in the `./data` directory.

## üß† Model Architecture
*   **Generator:**
    *   Takes a `latent_dim` (100 by default) random noise vector as input.
    *   Uses a series of `Linear` layers, `ReLU` activations, `Unflatten` (to reshape for convolutions), `Upsample` layers, `Conv2d` layers, and `BatchNorm2d` layers.
    *   The final layer uses a `Tanh` activation to output pixel values in the range \[-1, 1], suitable for the normalized CIFAR-10 images.
*   **Discriminator:**
    *   Takes a 32x32x3 image (either real or generated) as input.
    *   Uses a series of `Conv2d` layers, `LeakyReLU` activations, `Dropout` layers for regularization, and `BatchNorm2d` layers.
    *   `ZeroPad2d` is used to adjust dimensions for convolutions.
    *   The network flattens the features and passes them through a `Linear` layer.
    *   A final `Sigmoid` activation outputs a probability score (0 for fake, 1 for real).

## üèãÔ∏è Training
*   **Loss Function:** Binary Cross-Entropy Loss (`nn.BCELoss()`) is used for both the Generator and Discriminator.
*   **Optimizers:** Adam optimizer (`optim.Adam`) is used for both networks.
*   **Hyperparameters:**
    *   `latent_dim = 100`
    *   `lr = 0.0002` (learning rate)
    *   `beta1 = 0.5`, `beta2 = 0.999` (Adam optimizer parameters)
    *   `num_epochs = 10`
    *   `batch_size = 32`

## üñºÔ∏è Expected Output
*   During training, the notebook will print the Discriminator and Generator losses every 100 batches.
*   At the end of the training (after 10 epochs in this configuration), a 4x4 grid of 16 generated images will be displayed. Given the simplicity of the GAN and the limited training epochs, these images will likely be abstract and colorful, showing early signs of learning features from the CIFAR-10 dataset.

## üíª Technology Stack
*   Python
*   PyTorch
*   Torchvision
*   NumPy
*   Matplotlib
*   Google Colab (for execution environment)

## üéØ Target Audience
*   Students and developers new to Generative Adversarial Networks.
*   Individuals looking for a basic, understandable PyTorch GAN implementation.
*   Anyone interested in generating images from data.

## ‚úÖ Status
Completed demo.
